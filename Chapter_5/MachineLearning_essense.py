# 5.1.2 深度学习泛化的本质
# 就是将人类的认知或者说公认的认知判断通过数据集的方式来进行建模
# 把 MNIST 数据集的标签打乱，然后在打乱后的数据集上训练一个模型。
# 尽管输入与打乱后的标签之间毫无关系，但**训练损失下降得不错**，而这只是一个相对较小的模型。
# 当然，**验证损失不会随着时间的推移有任何改善**，因为在这种情况下不可能泛化。
from keras.datasets import mnist
import numpy as np
from tensorflow import keras
from keras import layers

(train_images, train_labels), _ = mnist.load_data()
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255
random_train_labels = train_labels[:]
np.random.shuffle(random_train_labels)
model = keras.Sequential([
    layers.Dense(512, activation="relu"),
    layers.Dense(10, activation="softmax")
])
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
model.fit(train_images, random_train_labels,
          epochs=100,
          batch_size=128,
          validation_split=0.2)

# 1. 流形假说：个人判断——在分类问题上是显然错误的，但是合理详见：《流行上的微积分》
# MNIST 分类器的输入（在预处理之前）是一个由 0 ～ 255 的整数组成的 28×28 数组。
# 因此，输入值的总数为 256 的 784 次幂，这远远大于宇宙中的原子数目。
# 但是，这些输入中只有少数看起来像是有效的 MNIST 样本，
# 也就是说，在所有可能的 28×28 uint8 数组组成的父空间中，真实的手写数字只占据一个很小的子空间。
# 更重要的是，这个子空间不仅仅是父空间中随机散布的一组点，而是高度结构化的。

# 有效手写数字的子空间是连续的：如果取一个样本并稍加修改，那么它仍然可以被识别为同一个手写数字。
# 其次，有效子空间中的所有样本都由穿过子空间的光滑路径连接。也就是说，如果你取两个随机的 MNIST 数字 A 和 B，
# 就会存在将 A 变形成 B 的一系列“中间”图像，其中每两幅相邻图像都非常相似（见图 5-7）。
# 在两个类别的边界附近可能会有一些模棱两可的形状，但这些形状看起来仍然很像数字。

# 用术语来说，手写数字在 28×28 uint8 数组的可能性空间中构成了一个流形（manifold）。
# 这个词看起来很高深，但其概念非常直观。
# “流形”是指某个父空间的低维子空间，它局部近似于一个线性空间（欧几里得空间）。
# 例如，平面上的光滑曲线就是二维空间中的一维流形，因为对于曲线上的每一点，
# 你都可以画出一条切线（曲线上的每一点都可以用直线来近似）。
# 三维空间中的光滑表面是一个二维流形，以此类推。
# 其中也意味着，信息形态在宇宙的信息空间结构中的不同分类对应的不同信息结构


# 流形假说意味着：
#  机器学习模型只需在其输入空间中拟合相对简单、低维、高度结构化的子空间（潜在流形）；
#  在其中一个流形中，总是可以在两个输入之间进行插值（interpolate），也就是说，
# 通过一条连续路径将一个输入变形为另一个输入，这条路径上的所有点都位于流形中。
# 能够在样本之间进行插值是理解深度学习泛化的关键。

# 2. 插值作为泛化的来源
# 如果你处理的是可插值的数据点，那么你可以开始理解前所未见的点，
# 方法是将其与流形中相近的其他点联系起来。换句话说，你可以仅用空间的一个样本来理解空间的整体。
# 你可以用插值来填补空白。
# 流形插值的应用理解：将两个人脸进行拟合合成一个同时可能会被识别成两个人脸的人脸
# 人类能够进行极端泛化（extreme generalization），这是由不同于插值的认知机制实现的，
# 包括抽象、世界的符号模型、推理、逻辑、常识，以及关于世界的固有先验知识——
# 我们通常称其为理性（reason），与直觉和模式识别相对。

# 3. 深度学习为何有效
# 还记得第 2 章中的皱纸团比喻吗？一张纸表示三维空间中的二维流形。
# 深度学习模型是一个工具，用于让“纸团”恢复平整，也就是解开潜在流形。
# 深度学习模型本质上是一条高维曲线——一条光滑连续的曲线，因为它需要是可微的。

# 事实上，如果对模型训练足够长的时间，那么它最终会仅仅记住训练数据，根本没有泛化能力。
# 我认为这句话体现了教育的意义***人如果反复地被教训一件事情，他就会确信这就是真理。
# 人们输入的数据在输入空间中形成一个高度结构化的低维流形，
# 这就是流形假说。随着梯度逐渐下降，模型曲线会平滑地拟合这些数据。
# 因此，在训练过程中会有一个中间点，此时模型大致接近数据的自然流形。

# ******我认为这里这个中间流形为什么能够体现最好的泛化能力其实也和儒家的“度”很有相似之处******

# 深度学习模型不仅具有足够的表示能力，还具有以下特性，使其特别适合学习潜在流形。
#  深度学习模型实现了从输入到输出的光滑连续映射。它必须是光滑连续的，
# 因为它必须是可微的（否则无法进行梯度下降）。这种光滑性有助于逼近具有相同属性的潜在流形。
#  深度学习模型的结构往往反映了训练数据中的信息“形状”（通过架构预设）。
# 更一般地说，深度神经网络以分层和模块化的方式组织学到的表示，这与自然数据的组织方式相呼应。

# 4. 训练数据至关重要
# 虽然深度学习确实很适合流形学习，但泛化能力更多是自然数据结构的结果，而不是模型任何属性的结果。
# 只有数据形成一个可以插值的流形，模型才能够泛化。
# ****特征包含的信息量越大、特征噪声越小，泛化能力就越强，因为输入空间更简单，结构也更合理。
# 数据管理和特征工程对于泛化至关重要。
# 此外，由于深度学习是曲线拟合，因此为了使模型表现良好，需要在输入空间的密集采样上训练模型。
# 这里的“密集采样”是指训练数据应该密集地覆盖整个输入数据流形。在决策边界附近尤其应该如此。
# 有了足够密集的采样，就可以理解新输入，方法是在以前的训练输入之间进行插值，
# 无须使用常识、抽象推理或关于世界的外部知识——这些都是机器学习模型无法获取的。
# **我认为这是，常识与抽象推理应该是在于其数据处理上的选择方向。

# 为了学到能够正确泛化的模型，必须对输入空间进行**密集采样**

# 如果无法获取更多数据，次优解决方法是调节模型允许存储的信息量，或者对模型曲线的平滑度添加约束。
# 如果一个神经网络只能记住几种模式或非常有规律的模式，那么优化过程将迫使模型专注于最显著的模式，
# 从而更可能得到良好的泛化。***这种降低过拟合的方法叫作正则化（regularization）。
